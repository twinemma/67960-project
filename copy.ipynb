{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8345,"status":"ok","timestamp":1765335452101,"user":{"displayName":"Raina Wu","userId":"15536526180523369952"},"user_tz":300},"id":"yEeTrx9ztcgy","outputId":"347bf230-8ce1-434f-a324-cef2c299395c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/.shortcut-targets-by-id/1dwgmS-Fk3Kgg-RWfFuIvr8z2CnGXc00h/rank-collapse\n","ðŸ–¥ï¸  Using device: cuda\n","   GPU: NVIDIA A100-SXM4-40GB\n","âœ… Setup complete!\n","âœ… Datasets defined:\n","   - Copy (easy, diagonal attention)\n","   - Reverse (medium, anti-diagonal)\n","   - Sort (hard, global reasoning)\n","   - Text (language modeling)\n","   - Image (classification)\n","âœ… Transformer architectures defined:\n","   - TINY: 2 layers, 64d, 4 heads (~0K params)\n","   - SMALL: 4 layers, 128d, 8 heads (~4K params)\n","   - MEDIUM: 6 layers, 256d, 8 heads (~12K params)\n","   - LARGE: 8 layers, 512d, 16 heads (~65K params)\n","âœ… Rank metrics defined:\n","   - Stable rank (Frobenius / Operator norm)\n","   - Entropy effective rank\n","   - Top-k energy fraction\n","   - Attention sparsity\n","âœ… Training system ready!\n","âœ… Visualization tools ready!\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","# Global project paths\n","PROJECT = \"/content/drive/MyDrive/rank-collapse\"\n","# PROJECT = \"/content/drive/MyDrive/rank-collapse\"\n","%cd {PROJECT}\n","\n","from utils import *"]},{"cell_type":"code","source":["def make_standard_scheduler(which_layers=\"all\"):\n","    return TrainingScheduler(\n","        which_layers=which_layers, base_func=cross_entropy,\n","        functions_at_times=[])\n","\n","def run_noise(task_name, dataset_size_name, arch_name, when, weight=1.0, steps=2000, batch_size=64,\n","                      lr=3e-4, weight_decay=0.01, out_dir=\"logs\", seed=42):\n","    \"\"\"\n","    Build dataset, model, scheduler for the given config and run run_experiment.\n","    Saves logs as npy in out_dir with descriptive name.\n","    Returns path to saved file.\n","    \"\"\"\n","    set_seed(seed)\n","    sizes = {\"small\": 1024, \"medium\": 4096, \"large\": 8192}\n","    dataset_size = sizes[dataset_size_name]\n","\n","    # create datasets\n","    ds_train, task_type, vocab_size, _ = create_dataset(\n","        task_name, seq_len=32, vocab_size=64, dataset_size=dataset_size,\n","        split=\"train\")\n","    ds_probe, _, _, _ = create_dataset(\n","        task_name, seq_len=32, vocab_size=64,\n","        dataset_size=min(512, dataset_size), split=\"train\")\n","\n","    # instantiate model\n","    vocab_size=max(32, vocab_size)\n","    seq_len=32\n","    arch_config = ARCHITECTURES[arch_name].copy()\n","    d_model = arch_config['d_model']\n","    nhead = arch_config['nhead']\n","    num_layers = arch_config['num_layers']\n","    add_noise = NoiseScheduler(1, 1, 2000)\n","    model = ConfigurableTransformer(vocab_size=vocab_size, d_model=d_model, nhead=nhead,\n","                                    num_layers=num_layers, dim_feedforward=arch_config['dim_feedforward'],\n","                                    seq_len=seq_len, use_pos=True, causal=(task_type!='classification'), add_noise=add_noise)\n","    model = model.to(device)\n","\n","    # build scheduler\n","    scheduler = make_standard_scheduler()\n","\n","    # run experiment; ensure run_experiment uses kwargs activations, layers, and model\n","    log_name = f\"{task_name}_{dataset_size_name}_{arch_name}_noise_{when}_steps{steps}_seed{seed}.npy\"\n","    os.makedirs(out_dir, exist_ok=True)\n","    out_path = os.path.join(out_dir, log_name)\n","\n","    model, _ = run_experiment((ds_train, task_type, None, None),\n","                (ds_probe, task_type, None, None),\n","                model, arch_config,\n","                steps=steps, batch_size=batch_size,\n","                checkpoint_every=max(1, steps // 20), lr=lr,\n","                seed=seed, loss_scheduler=scheduler, metric_func=full_spectrum_metrics,\n","                log_store_name=out_path, weight_decay=weight_decay)\n","\n","    print(f\"Saved logs to {out_path}\")\n","    return out_path\n","\n","def run_normal(task_name, dataset_size_name, arch_name, when, weight=1.0, steps=2000, batch_size=64,\n","                      lr=3e-4, weight_decay=0.01, out_dir=\"logs\", seed=42):\n","    \"\"\"\n","    Build dataset, model, scheduler for the given config and run run_experiment.\n","    Saves logs as npy in out_dir with descriptive name.\n","    Returns path to saved file.\n","    \"\"\"\n","    set_seed(seed)\n","    sizes = {\"small\": 1024, \"medium\": 4096, \"large\": 8192}\n","    dataset_size = sizes[dataset_size_name]\n","\n","    # create datasets\n","    ds_train, task_type, vocab_size, _ = create_dataset(\n","        task_name, seq_len=32, vocab_size=64, dataset_size=dataset_size,\n","        split=\"train\")\n","    ds_probe, _, _, _ = create_dataset(\n","        task_name, seq_len=32, vocab_size=64,\n","        dataset_size=min(512, dataset_size), split=\"train\")\n","\n","    # instantiate model\n","    vocab_size=max(32, vocab_size)\n","    seq_len=32\n","    arch_config = ARCHITECTURES[arch_name].copy()\n","    d_model = arch_config['d_model']\n","    nhead = arch_config['nhead']\n","    num_layers = arch_config['num_layers']\n","    model = ConfigurableTransformer(vocab_size=vocab_size, d_model=d_model, nhead=nhead,\n","                                    num_layers=num_layers, dim_feedforward=arch_config['dim_feedforward'],\n","                                    seq_len=seq_len, use_pos=True, causal=(task_type!='classification'))\n","    model = model.to(device)\n","\n","    # build scheduler\n","    scheduler = make_standard_scheduler()\n","\n","    # run experiment; ensure run_experiment uses kwargs activations, layers, and model\n","    log_name = f\"{task_name}_{dataset_size_name}_{arch_name}_normal_{when}_steps{steps}_seed{seed}.npy\"\n","    os.makedirs(out_dir, exist_ok=True)\n","    out_path = os.path.join(out_dir, log_name)\n","    if not os.path.exists(out_path):\n","        model, _ = run_experiment((ds_train, task_type, None, None),\n","                    (ds_probe, task_type, None, None),\n","                    model, arch_config,\n","                    steps=steps, batch_size=batch_size,\n","                    checkpoint_every=max(1, steps // 20), lr=lr,\n","                    seed=seed, loss_scheduler=scheduler, metric_func=full_spectrum_metrics,\n","                    log_store_name=out_path, weight_decay=weight_decay)\n","\n","        print(f\"Saved logs to {out_path}\")\n","    else:\n","        print(f\"{out_path} has already been done\")\n","    return out_path\n","\n","tasks = [\"copy\", \"sort\", \"tiny_stories\"]\n","archs = [\"small\", \"medium\", \"large\"]\n","dataset_sizes = [\"medium\", \"large\"]\n","whens = [\"all\"]\n","\n","for task in tasks:\n","    for size_name in dataset_sizes:\n","        for arch in archs:\n","            for when in whens:\n","                print(f\"RUN: task={task} size={size_name} arch={arch} when={when}\")\n","                run_noise(task, size_name, arch, when,\n","                        steps=2000, batch_size=64, out_dir=\"logs\", seed=42)"],"metadata":{"id":"YIjUIqtqLQlc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_single_config(task_name, dataset_size_name, arch_name, reg_name,\n","                      reg_func, when, weight=1.0, steps=2000, batch_size=64,\n","                      lr=3e-4, weight_decay=0.01, out_dir=\"logs\", seed=42):\n","    \"\"\"\n","    Build dataset, model, scheduler for the given config and run run_experiment.\n","    Saves logs as npy in out_dir with descriptive name.\n","    Returns path to saved file.\n","    \"\"\"\n","    set_seed(seed)\n","    sizes = {\"small\": 1024, \"medium\": 4096, \"large\": 8192}\n","    dataset_size = sizes[dataset_size_name]\n","\n","    # create datasets\n","    ds_train, task_type, vocab_size, _ = create_dataset(\n","        task_name, seq_len=32, vocab_size=64, dataset_size=dataset_size,\n","        split=\"train\")\n","    ds_probe, _, _, _ = create_dataset(\n","        task_name, seq_len=32, vocab_size=64,\n","        dataset_size=min(512, dataset_size), split=\"train\")\n","\n","    # instantiate model\n","    model, arch_config = instantiate_architecture(\n","        arch_name, vocab_size=max(32, vocab_size), seq_len=32, task_type=task_type)\n","    model = model.to(device)\n","\n","    # build scheduler\n","    scheduler = make_scheduler_for_regularizer(\n","        reg_func, steps=steps, when=when, which_layers=\"all\", weight=weight)\n","\n","    # run experiment; ensure run_experiment uses kwargs activations, layers, and model\n","    log_name = f\"{task_name}_{dataset_size_name}_{arch_name}_{reg_name}_{when}_steps{steps}_seed{seed}.npy\"\n","    os.makedirs(out_dir, exist_ok=True)\n","    out_path = os.path.join(out_dir, log_name)\n","\n","    if not os.path.exists(log_name):\n","        model, _ = run_experiment((ds_train, task_type, None, None),\n","                    (ds_probe, task_type, None, None),\n","                    model, arch_config,\n","                    steps=steps, batch_size=batch_size,\n","                    checkpoint_every=max(1, steps // 20), lr=lr,\n","                    seed=seed, loss_scheduler=scheduler, metric_func=full_spectrum_metrics,\n","                    log_store_name=out_path, weight_decay=weight_decay)\n","        print(f\"Saved logs to {out_path}\")\n","    else:\n","        print(f\"Already saved to {out_path}\")\n","    return out_path\n"],"metadata":{"id":"P86jyOjcCNb8","executionInfo":{"status":"ok","timestamp":1765335456161,"user_tz":300,"elapsed":6,"user":{"displayName":"Raina Wu","userId":"15536526180523369952"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"VjJJpoNJtxpa","outputId":"36246b06-27ec-4819-cdd7-294a577c205b","executionInfo":{"status":"ok","timestamp":1765335908636,"user_tz":300,"elapsed":450234,"user":{"displayName":"Raina Wu","userId":"15536526180523369952"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 1024). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["Model: 8 layers, 512d, 16 heads\n","Parameters: 76,995,665\n","Device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [07:26<00:00,  4.48step/s, loss=4.9808, acc=0.2292]"]},{"output_type":"stream","name":"stdout","text":["âœ… Training complete in 446.1s\n","Saved logs to logs/tiny_stories_medium_large_logdet_all_steps2000_seed42.npy\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["'logs/tiny_stories_medium_large_logdet_all_steps2000_seed42.npy'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["run_single_config(\"tiny_stories\", \"medium\", \"large\", \"logdet\", increase_rank_regularizers[\"logdet\"], \"all\")"]},{"cell_type":"code","source":["run_single_config(\"tiny_stories\", \"medium\", \"large\", \"erank\", increase_rank_regularizers[\"erank\"], \"all\")"],"metadata":{"id":"UjNqAV3fMBDQ","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1765336795340,"user_tz":300,"elapsed":886698,"user":{"displayName":"Raina Wu","userId":"15536526180523369952"}},"outputId":"5f74eb56-01d2-4eda-f9ca-cfe89fb3ee36"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: 8 layers, 512d, 16 heads\n","Parameters: 76,995,665\n","Device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [14:42<00:00,  2.27step/s, loss=4.1182, acc=0.2550]"]},{"output_type":"stream","name":"stdout","text":["âœ… Training complete in 882.8s\n","Saved logs to logs/tiny_stories_medium_large_erank_all_steps2000_seed42.npy\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["'logs/tiny_stories_medium_large_erank_all_steps2000_seed42.npy'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["run_single_config(\"tiny_stories\", \"medium\", \"large\", \"orthonorm\", increase_rank_regularizers[\"orthonorm\"], \"all\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"T3I55alDMgaX","executionInfo":{"status":"ok","timestamp":1765337215103,"user_tz":300,"elapsed":419660,"user":{"displayName":"Raina Wu","userId":"15536526180523369952"}},"outputId":"c7f28b3b-6bf2-41dd-e3f8-abe765ca6ef9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: 8 layers, 512d, 16 heads\n","Parameters: 76,995,665\n","Device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [06:55<00:00,  4.81step/s, loss=2.7402, acc=0.4253]"]},{"output_type":"stream","name":"stdout","text":["âœ… Training complete in 415.9s\n","Saved logs to logs/tiny_stories_medium_large_orthonorm_all_steps2000_seed42.npy\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["'logs/tiny_stories_medium_large_orthonorm_all_steps2000_seed42.npy'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["run_single_config(\"tiny_stories\", \"medium\", \"large\", \"spectral_norm\", decrease_rank_regularizers[\"spectral_norm\"], \"all\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"rxUi1dJoMbvD","executionInfo":{"status":"ok","timestamp":1765338861777,"user_tz":300,"elapsed":1351285,"user":{"displayName":"Raina Wu","userId":"15536526180523369952"}},"outputId":"e7565ecd-4183-43c1-8b8b-440baf985c83"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: 8 layers, 512d, 16 heads\n","Parameters: 76,995,665\n","Device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [22:27<00:00,  1.48step/s, loss=2.7461, acc=0.4255]"]},{"output_type":"stream","name":"stdout","text":["âœ… Training complete in 1347.6s\n","Saved logs to logs/tiny_stories_medium_large_spectral_norm_all_steps2000_seed42.npy\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["'logs/tiny_stories_medium_large_spectral_norm_all_steps2000_seed42.npy'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":[],"metadata":{"id":"XzFhUBTIMjBE"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}